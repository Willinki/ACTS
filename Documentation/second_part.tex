\section{Second part}
The second part of the algorithm aims at calculating, for each unlabelled instance,
its \textbf{uncertainty} and \textbf{utility}.

First we'll define some thing that we have until now:
\begin{itemize}
    \item Instances. The labelled examples until now, we'll refer to these instances as $Y_i$, they are
    contained inside $D_L$, each one of these instances has a label and an index, we'll refer to these as 
    $Y_i.l$, $Y_i.i$. Also, each instance is assigned to a pattern, we'll refer to this patterns as $Y_i.pt$.
    \item Patterns (or shapelets). Short subsequences that represents part of the instances, each instance is 
    assigned to a pattern. Each pattern has its own $\lambda$ parameter, which is used to calculate the
    quantity $P(Y | pt)$. Also, each parameter has its own $P(pt|y = l)$ for each $l$ inside the possible labels.
    \item Unlabelled instances. We'll refer to these as $X_i$, contained inside $D_U$.
    \item We also have a distance function between time series, or time series and patterns: \texttt{\_dis(Y, X)}.
    \item We also have a \textbf{label set} $L$, i.e. the list of all possible labels inside the dataset. 
    If the classification is binary the label set will be $L = \{0, 1\}$, for example.
\end{itemize}

\subsection{Uncertainty calculation}
The first quantity we have to measure is the uncertainty related to an instance $X \in D_U$.

The steps are: \textbf{for each instance X}:
\begin{itemize}
    \item Calculate its k-nearest neighbors in $D_L$. We'll refer to these as 
    $\{Y_j\}_{j\in 1\dots k}$, so ${Y_1, \dots, Y_k}$ where $Y_1$ is the closest instance and $Y_k$ the 
    farthest of the k\-nn.
    From this, we also need to retrieve the following quantities:
    \begin{equation}
        d_1 = dis(X, Y_1) 
    \end{equation}
    \begin{equation}
        d_k = dis(X, Y_k) 
    \end{equation}

    \item  Calculate, for all possible $l \in L$ the quantity:
    \begin{equation}
        \bar{P}(y = l | X) = \sum_{Y_j \in \{Y_1, \dots, Y_k\}} P(X | Y_j.pt) \cdot P(Y_j.pt | y = l)
        \label{eq:eq_utile}
    \end{equation}
    Where the both term are available: the function \texttt{calculate\_probax(X, pt)} calculates the 
    first part, while the second term is contained in the \texttt{l\_probas} column of the 
    \textit{patterns} dataframe. It is an array where the first entry is $P(Y_j.pt | y = 1)$ and 
    so on for each possible label.
    \item Then we calculate the normalizer:
    \begin{equation}
        Z = \sum_{l \in L} \bar{P}(y = l | X)
    \end{equation}
    \item Then we normalize the quantities we just calculated with $Z$, obtaining:
    \begin{equation}
        \hat{P}(y = l | X) = \frac{1}{Z}  \bar{P}(y = l | X)
    \end{equation}
    \item Then finally we calculate the uncertainty for the unlabelled instance $X$:
    \begin{equation}
        Uncr(X) = \sum_{l \in L} \hat{P}(y = l | X) \log \left( 
                \hat{P}(y = l | X)
        \right) \frac{d_1}{d_k}
    \end{equation}
\end{itemize}

\subsection{Utility calculation}
The utility needs to be calculated for each instance $X_i \in D_u$, but needs the so called 
\textit{reverse nearest neighbors} of X, which are the instances $\{Y_1, \dots, Y_n\}$, 
that have $X_i$ as a k nearest neighbor.

To do this:
\begin{itemize}
    \item We calculate, for each instance $Y_i \in D_L$, it's k nearest neighbors in $D_U$.
    \item For each instance $X \in D_U$, we check all the instances $Y_j \in D_L$ that have $X_i$ 
    as a knn
\end{itemize}

This way we obtain the set of reverse nearest neighbors of $X_i$, which we'll call $RN(X_i)$. 
We'll identify the instances in this set with $\{Y_1 , \dots, Y_k\}$.

Now, we calculate the following quantities:
\begin{equation}
    dis(X_i, Y_j) 
    \, \, \, \, \, \, \,\,   \, \, 
    \text{for } Y_j \in RN(X_i)
\end{equation}

Of which we'll calculate:
\begin{equation}
    mDis(X_i) = \max_j dis(X_i, Y_j) 
    \, \, \, \, \, \, \,\,   \, \, 
    \text{for } Y_j \in RN(X_i)
\end{equation}

And then:
\begin{align}
    SimD(X_i, Y_j) = 1 - \frac{
        dis(X_i, Y_j)
    }{
        mDis(X_i)
    } 
    \, \, \, \, \, \, \,\,   \, \, 
    \text{for } Y_j \in RN(X_i)
\end{align}

Now the first part of the utility is done, we have to calculate the second part.

To do this, we'll calculate the nearest neighbors of $X_i$ in $D_L$.
We'll refer to these as $\{Y_1, \dots, Y_k\} \in LN(X_i)$.

Now, we have to calculate the follwing quantity for each possible pattern:
\begin{equation}
    \Psi(X_i, pt) = \sum_{Y_j \in LN(X_i)} P(X_i | Y_j.pt) I(Y_j.pt = pt)
    \, \, \, \, \, \, \,\,   \, \, 
    \text{for } pt \in patterns
\end{equation}

Where the first term is the same of Equation \ref{eq:eq_utile} first part, so available 
with the function \texttt{calculate\_probax(X, pt)}, where pt will be the pattern
assigned to $Y_j$.

The second part, on the other hand is defined as follows:
\begin{equation}
    I(Y_j.pt = pt)
    \begin{cases}
        0 & \text{if } Y_j.pt \text{ is not the same as } pt \\
        1 & \text{if } Y_j.pt \text{ is the same as } pt
    \end{cases}
\end{equation}

For the values of $\Psi(X_i, pt)$ calculated for each pt, we calculate:
\begin{equation}
    Z(X_i|PT) = \sum_{pt} \Psi(X_i, pt)
\end{equation}

Now we have that:
\begin{equation}
    P(X_i | pt) = Z(X_i|PT)^{-1} \Psi(X_i, pt)
\end{equation}

This element is the basic element for the calculation of the utility, it must be
defined also for a labelled instance $Y_i \in D_L$, so, to calculate it:
\begin{itemize}
    \item Find the knn of $Y_j$ in $D_L$, which will be referred to as $LN(Y_j)$.
    \item Calculate:
    \begin{align}
        \Psi(Y_j, pt) = \sum_{Y_k \in LN(Y_j)} P(Y_j | Y_k.pt) I(Y_j.pt = pt)
        \, \, \, \, \, \, 
        \text{for all } pt \in patterns 
    \end{align}
    \item Calculate:
    \begin{equation}
        Z(Y_j | PT) = \sum_{pt} \Psi(Y_j, pt)
    \end{equation}
    \item Calculate:
    \begin{equation}
        P(Y_j | pt) = Z(Y_j |PT )^{-1} \Psi(Y_j, pt)
    \end{equation}
\end{itemize}

Now, let suppose that all the possible patterns are $[pt_1, pt_2, \dots, pt_n]$, 
we define the following arrays as:
\begin{align}
    P(X_i | PT) &= \left[ P(X_i | pt_1), \dots, P(X_i | pt_n) \right]\\
    P(Y_j | PT) &= \left[ P(Y_j | pt_1), \dots, P(Y_j | pt_n) \right] 
\end{align}

And we introduce the following quantity:
\begin{equation}
    SimP(X_i, Y_j) = 1 - JSD(P(X_i | PT), P(Y_i | PT))
\end{equation}

Where JSD is the Jensen Shannon Distance between the arrays, available 
in \texttt{scipy}.

Then:
\begin{equation}
    Sim(X_i, Y_j) = SimD(X_i, Y_j) \cdot SimP(X_i, Y_j)
\end{equation}

Lastly, once we defined everything, and remembering that $RN(X_i)$ is the set of reverse
k nearest neighbors in $D_L$ of $X_i \in D_U$, we define the utility of an instance
\begin{equation}
    Uti(X_i) = \sum_{Y_j \in RN(X_i)} Sim(X_i, Y_j)
\end{equation}

And its question informativeness:
\begin{equation}
    QI(X_i) = Uti(X_i) + Uncr(X_i)
\end{equation}

We select the n most informative instances to query.




